require(rtweet)
require(quanteda)
require(ggplot2)
tweets <-search_tweets("terms to search", n=18000, include_rts = FALSE)##please replace "terms to search with actual terms you want to search online!
corpusea<- corpus(terms to search)
ea_dfm<- dfm(terms to search remove_punct=TRUE)
stopwords(language = "en", source = "snowball", simplify = TRUE)


##checking frequency of posting (this is relevant only if you get a weekly (or bigger) dataset
Terms to search %>%
  ts_plot("here input the time such as weekly, 3 hours, one day etc..") + ##All that isin "" has to be replaced with actual time :)
  ggplot2::theme_minimal() +
  ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
  ggplot2::labs(
    x = NULL, y = NULL,
    title = "Frequency of #somethingsomething Twitter statuses from day 1 to day 12",
    subtitle = "Twitter stories",
    caption = "\nSource: Data collected from Twitter's REST API via rtweet"
  )

##getting a network of most used tags/keywords and hashtags ##the latter one is especially useful if you have searched keywords rather than hashtags or if the tweets present multiple hashtags attached tothe one you were researching!

tag_dfm<-dfm_select(ea_dfm)
head(tag_dfm)
topgat_fcm <- fcm_select(tag_fcm, pattern = toptag)
head(tag_fcm)
toptag<-names(topfeatures(tag_dfm,15))
head(toptag)
topgat_fcm<- fcm_select(tag_fcm,pattern=toptag)
textplot_network(topgat_fcm, min_freq = 0.1, edge_alpha = 0.8, edge_size = 5)
toks<- ea_dfm%>%
  + tokens(remove_punct = TRUE) %>%
  + tokens_remove(stopwords("english"), padding = FALSE)
 fcmat <- fcm(toks, context = "window", tri = FALSE)
 feat <- names(topfeatures(fcmat, 30))
 fcm_select(fcmat, pattern = feat) %>%
  +     textplot_network(min_freq = 0.5)
  
  ##the result should be a nice network. Happy deciphering!
